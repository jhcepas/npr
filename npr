#!/usr/bin/env python
import os
import time
import imp
import logging
from collections import deque
from argparse import ArgumentParser, RawDescriptionHelpFormatter
from configobj import ConfigObj
from validate import Validator

from master_task import Task

import sys
sys.path.insert(0, "/home/jhuerta/_Devel/ete/2.2/")
from ete_dev import SeqGroup


try:
    module_path = os.path.split(__file__)[0]
    __VERSION__ = open(os.path.join(module_path, "VERSION")).read().strip()
except: 
    __VERSION__ = "unknown"

try:
    module_path = os.path.split(__file__)[0]
    __DATE__ = open(os.path.join(module_path, "DATE")).read().strip()
except: 
    __DATE__ = "unknown"


__DESCRIPTION__ = """ 
--------------------------------------------------------------------------------
                 Nested Phylogenetic Reconstruction program.  
                          NPR-0.1%s(alpha), %s.

      NPR is a program to reconstruct phylogenetic trees by optimizing
      each of the internal node.

      If you use this program for published work, please cite: 

       Jaime Huerta-Cepas and Toni Gabaldon. Nested Phylogenetic
       Reconstruction. XXX-XX. 2011.

      Contact: jhuerta (at) crg.es, tgabaldon (at) crg.es
--------------------------------------------------------------------------------
""" %(__VERSION__, __DATE__)

RETRY_WHEN_ERRORS = True
EXECUTE = True

__LOGINDENT__ = 0
def set_logindent(x):
    global __LOGINDENT__
    __LOGINDENT__ = x

def inc_logindent(x):
    global __LOGINDENT__
    __LOGINDENT__ += x

def dec_logindent(x):
    global __LOGINDENT__
    __LOGINDENT__ -= x

class IndentedFormatter(logging.Formatter):
    def __init__( self, fmt=None, datefmt=None ):
        logging.Formatter.__init__(self, fmt, datefmt)

    def format( self, rec ):
        rec.indent = ' '*__LOGINDENT__
        out = logging.Formatter.format(self, rec)
        return out

def schedule(config, processer, schedule_time):
    """ Main pipeline scheduler """ 
    WAITING_TIME = schedule_time
    # Pass seed files to processer to generate the initial task
    pending_tasks, main_tree = processer(None, None, 
                                         config)

    # Then enters into the pipeline. 
    while pending_tasks:
        for task in list(pending_tasks):
            set_logindent(0)
            log.info(task)
            inc_logindent(2)
            log.info("TaskDir: %s" %task.taskdir)
            log.info("TaskJobs: %d" %len(task.jobs))
            inc_logindent(2)
            for j in task.jobs:
                log.info(j)
            inc_logindent(-2)

            if task.status == "W":
                task.dump_job_commands()
                task.status = "R"
                if EXECUTE: 
                    log.info("Running jobs..")
                    os.system("sh %s" %task.jobs_file)

            if task.status == "R":
                log.info("Task is marked as Running")
                jobs_status = task.get_jobs_status()
                log.info("JobStatus: %s" %jobs_status)
                if jobs_status == set("D"):
                    task.finish()
                    if task.check():
                        inc_logindent(3)
                        new_tasks, main_tree = processer(task, main_tree, 
                                                         config)
                        inc_logindent(-3)
                        pending_tasks.extend(new_tasks)
                        pending_tasks.remove(task)
                        task.status = "D"
                    else: 
                        log.error("Task looks done but result files are not found")
                        task.status = "E"
                elif "E" in jobs_status:
                    task.status = "E"

            elif task.status == "E":
                log.info("Task is marked as ERROR")
                if RETRY_WHEN_ERRORS:
                    log.info("Remarking task as undone to retry")
                    task.retry()
            elif task.status == "D":
                log.info("Task is DONE")
            
        time.sleep(WAITING_TIME)
        print

if __name__ == "__main__":
    parser = ArgumentParser(description=__DESCRIPTION__, 
                            formatter_class=RawDescriptionHelpFormatter)
    # name or flags - Either a name or a list of option strings, e.g. foo or -f, --foo.
    # action - The basic type of action to be taken when this argument is encountered at the command line. (store, store_const, store_true, store_false, append, append_const, version)
    # nargs - The number of command-line arguments that should be consumed. (N, ? (one or default), * (all 1 or more), + (more than 1) )
    # const - A constant value required by some action and nargs selections. 
    # default - The value produced if the argument is absent from the command line.
    # type - The type to which the command-line argument should be converted.
    # choices - A container of the allowable values for the argument.
    # required - Whether or not the command-line option may be omitted (optionals only).
    # help - A brief description of what the argument does.
    # metavar - A name for the argument in usage messages.
    # dest - The name of the attribute to be added to the object returned by parse_args().

    parser.add_argument("-c", "--config", dest="configfile", 
                        type=str, required=True, 
                        help="""A valid config file to execute the pipeline""")

    parser.add_argument("-a", "--aa-seed-file", dest="aa_seed_file", 
                        type=str, 
                        help="""Initial multi sequence file with protein sequences""")

    parser.add_argument("-n", "--nt-seed-file", dest="nt_seed_file", 
                        type=str, 
                        help="""Initial multi sequence file with nucleotide sequences""")

    parser.add_argument("-t", "--schedule-time", dest="schedule_time", 
                        type=int, default=1,
                        help="""Schedule time in seconds. """)

    parser.add_argument("-v", "--verbose", dest="verbose", 
                        action="store_true",
                        help="""Print all info messages """)

    parser.add_argument("-r", "--retry-error-jobs", dest="retry_error_jobs", 
                        action="store_true",
                        help="""Try to relaunch jobs marked as error """)

    parser.add_argument("-x", "--execute-inplace", dest="execute-inplace", 
                        action="store_true",
                        help="""Jobs are launched in this machine """)
 
    args = parser.parse_args()

    print __DESCRIPTION__    

    # Prepares main log
    log = logging.getLogger("main")
    log.setLevel(logging.DEBUG)
    log_format = IndentedFormatter("%(levelname) 7s -%(indent)s %(message)s")
    log_handler = logging.StreamHandler()
    log_handler.setFormatter(log_format)
    log.addHandler(log_handler)

  
    configspec = ConfigObj("specs", 
                           list_values=False, _inspec=True)

    config = ConfigObj("config", configspec=configspec)
    
    val = Validator()
    if not config.validate(val):
        print test
        raise 

    base_dir = os.path.abspath(config["general"]["basedir"])
    if not os.path.exists(base_dir):
        os.mkdir(base_dir)

    config["general"]["basedir"] = base_dir
    Task.global_config["basedir"] = base_dir
    # Load pipeline
    pipeline_module = config["general"]["pipeline"]
    if os.path.isfile(pipeline_module):
        template = imp.load_module("__main__.standard1", open(pipeline_module),
                                   pipeline_module, ["py", "r", imp.PY_SOURCE])
    else:
        imp.find_module("standard1", __path__)


    # check nt_seed and aa_seed. Do they contain the same seqs?
    # format? names?
    visited_seqs = set()
    seed_files = {}
    for label, seq_file in [["aa_seed", args.aa_seed_file],
                            ["nt_seed", args.nt_seed_file]]:
        if seq_file: 
            seqs = SeqGroup(seq_file)
            for i,n in seqs.id2name.iteritems():
                if len(n)>10: 
                    seqs.id2name[i] = n[:10]
                    log.warning("sequence names %s was truncated to 10 chars: %s " %(n, n[:10]))
            values = seqs.id2name.values()
            if len(values) != len(set(values)):
                raise Exception("Duplicated sequence names, execution aborted.")
            if visited_seqs and set(values) != visited_seqs:
                raise Exception("Sequence names do not match between aa and nt files.")
            visited_seqs.update(values)
            fullpath = os.path.join(base_dir, label+".fasta")
            seed_files[label] = fullpath
            seqs.write(outfile=fullpath)

    config["general"]["aa_seed"] =  seed_files.get("aa_seed", None)
    config["general"]["nt_seed"] =  seed_files.get("nt_seed", None)

    schedule(config, template.pipeline, args.schedule_time)

        
        
