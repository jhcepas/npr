#!/usr/bin/env python
import sys
import os
import shutil
from StringIO import StringIO
from collections import defaultdict
import filecmp
import logging
log = None
import numpy
import time
# This avoids installing nprlib module. npr script will find it in the
# same directory in which it is
NPRPATH = os.path.split(os.path.realpath(__file__))[0]

sys.path.insert(0, NPRPATH)
from nprlib import argparse
from nprlib.configobj import ConfigObj, flatten_errors
from nprlib import validate
from nprlib.utils import strip, SeqGroup, generate_runid,  AA, NT, GLOBALS, encode_seqname
from nprlib.errors import ConfigError, DataError
from nprlib.master_task import Task
from nprlib.interface import app_wrapper
from nprlib.scheduler import schedule
from nprlib import db
from nprlib import apps
from nprlib.template.common import CONFIG_SPECS
try:
    module_path = os.path.split(__file__)[0]
    __VERSION__ = open(os.path.join(module_path, "VERSION")).read().strip()
except:
    __VERSION__ = "unknown"

try:
    module_path = os.path.split(__file__)[0]
    __DATE__ = open(os.path.join(module_path, "DATE")).read().strip()
except:
    __DATE__ = "unknown"

__DESCRIPTION__ = (
"""--------------------------------------------------------------------------------
                 Nested Phylogenetic Reconstruction program.
                          NPR-0.1%s(beta), %s.

      NPR allows to reconstruct large phylogenies by means of an
      iterative strategy that provides scalable resolution along the
      different levels of a tree. At each iteration, internal nodes
      are re-evaluated to optimize and fine tune the parameters of the
      phylogenetic inference.

      If you use this program in a published work, please cite:

        Jaime Huerta-Cepas and Toni Gabaldon. Nested Phylogenetic
        Reconstruction. XXX-XX. 2012.

      (Note that a list of external programs used to complete the
      necessary computations will be shown shown together with your
      results.)

      Contact: jhuerta [at] crg.es & tgabaldon [at] crg.es
--------------------------------------------------------------------------------

    """ %(__VERSION__, __DATE__))

def existing_file(value):
    if os.path.isfile(value):
        return value
    else:
        raise ConfigError("Not valid file")

def check_number(value, cast, minv=0, maxv=None):
    try:
        typed_value = cast(value)
    except ValueError:
        raise ConfigError("Expected [%s] number. Found [%s]" %(cast, value))
    else:
        if (minv is not None and typed_value < cast(minv)) or \
           (maxv is not None and typed_value > cast(maxv)):
            _minv = minv if minv is not None else "any"
            _maxv = maxv if maxv is not None else "any"
            raise ConfigError("[%s] not in the range (%s,%s)" %
                              (value, _minv, _maxv))
    return typed_value

def is_float(value, minv=None, maxv=None):
    return check_number(value, float, minv, maxv)

def is_integer(value, minv=None, maxv=None):
    return check_number(value, int, minv, maxv)
    
def is_list(value):
    if not isinstance(value, list):
        raise ConfigError("Expected a list of values. Found [%s]" %value)
    return value

def is_boolean_list(value):
    is_list(value)
    typed_value = []
    for v in value:
        typed_value.append(check_number(v, bool, 0, 1))
    return typed_value

def is_float_list(value, minv=0, maxv=None):
    is_list(value)
    typed_value = []
    for v in value:
        typed_value.append(check_number(v, float, minv, maxv))
    return typed_value

def is_integer_list(value, minv=0, maxv=None):
    is_list(value)
    typed_value = []
    for v in value:
        typed_value.append(check_number(v, int, minv, maxv))
    return typed_value

def is_correlative_integer_list(value, minv=1, maxv=None):
    is_list(value)
    typed_value = []
    last_value = 0
    for v in value:
        typed_value.append(check_number(v, int, minv, maxv))
        if v <= last_value:
            return False
        last_value = v
    return typed_value
    
def main(args):
    global log
    base_dir = GLOBALS["basedir"]
    log = logging.getLogger("main")
    print __DESCRIPTION__

    local_conf_file = os.path.join(base_dir, "pipeline.cfg")
    if os.path.exists(base_dir):
        if os.path.exists(local_conf_file):
            if not args.override and \
               not filecmp.cmp(args.configfile, local_conf_file):
                raise ConfigError("Output directory seem to contain"
                                  " a NPR execution using a different"
                                  " config file [pipeline.cfg]. Use"
                                  " --override option or change the"
                                  " output path.")

    try:
        config = ConfigObj(args.configfile, list_values=True,
                           configspec=StringIO(CONFIG_SPECS))
    except Exception, e:
        raise ConfigError(str(e))

    npr_template = args.workflow 
    if npr_template == "sptree":
        from nprlib.template.sptree import pipeline
    elif npr_template == "genetree":
        from nprlib.template.genetree import pipeline
    else:
        raise ConfigError("Bad config file. A template name or file should be specified.")

    custom_types = {"boolean_list": is_boolean_list,
                    "integer_list": is_integer_list,
                    "correlative_integer_list": is_correlative_integer_list,
                    "float_list": is_float_list,
                    "float": is_float,
                    "integer": is_integer,
                    "list": is_list,
                    }
    val = validate.Validator(custom_types)
    check = config.validate(val)
    if not check:
        print flatten_errors(config, check)
        raise ConfigError("Errors found when parsing config file")

    # Check that fields are filled for all max_seq values
    for flow_name in ["sptree", "genetree"]:
        npr_config = [len(v) for k, v in config[flow_name].iteritems()
                      if type(v) == list]
        if len(set(npr_config)) != 1:
            raise ConfigError("Unequal length of main NPR config table.")

    if args.arch == "auto":
        if sys.maxsize > 2**32:
            arch = "64"
        else:
            arch = "32"
    else:
        arch = args.arch

    APPSPATH = os.path.join(NPRPATH, "apps/x86-%s" %arch)
    # setup portable apps
    for k,v in config["app"].iteritems():
        if v == "built-in":
            cores = int(config["threading"].get(k, args.maxcores))
            cores = min(args.maxcores, cores)
            config["threading"][k] = cores
            if args.sge_execute:
                _apps_path = os.path.realpath(config["sge"]["remote_npr_path"])
                cmd = apps.get_call(k, _apps_path, base_dir, cores)
                config["app"][k] = cmd
            else:
                cmd = apps.get_call(k, APPSPATH, base_dir, str(cores))
                config["app"][k] = cmd
    #log.debug('\n'.join(["%s\t%s" %(i,j) for i,j in config["app"].items()]))
    if not args.nochecks:
        print "Testing x86-%s portable applications..." % arch
        apps.test_apps(config["app"])

    # Set number of CPUs available
    GLOBALS["_max_cores"] = args.maxcores
    log.debug("Enabling %d CPU cores" %args.maxcores)

    # Create dir structure
    gallery_dir = os.path.join(base_dir, "gallery")
    sge_dir = os.path.join(base_dir, "sge_jobs")
    tmp_dir = os.path.join(base_dir, "tmp")
    tasks_dir = os.path.join(base_dir, "tasks")
    GLOBALS["sge_dir"] = sge_dir
    GLOBALS["tmp"] = tmp_dir
    GLOBALS["gallery_dir"] = gallery_dir
    GLOBALS["tasks_dir"] = tasks_dir
    for dirname in [gallery_dir, sge_dir, tmp_dir, tasks_dir]:
        try:
            os.makedirs(dirname)
        except OSError:
            log.warning("Using existing dir: %s", dirname)

    # Copy config file
    shutil.copy(args.configfile, os.path.join(base_dir, "pipeline.cfg"))

    if npr_template == "sptree" and not args.ortho_pairs:
        raise ConfigError("Species tree workflow requires a list"
                          " of orthologous gene/protein pairs to be"
                          " supplied through the --ortho-pair"
                          " argument.")

    GLOBALS["seqtypes"] = set()
    if args.aa_seed_file:
        GLOBALS["seqtypes"].add("aa")
    if args.nt_seed_file:
        GLOBALS["seqtypes"].add("nt")

    if npr_template == "sptree" and args.spfile:
        GLOBALS["target_species"] = set([line.strip() for line in open(args.spfile)])
        

    GLOBALS["db_file"]  = os.path.join(base_dir, "db.sqlite3")
    if args.cleardb and os.path.exists(GLOBALS["db_file"]):
        log.log(28, "Erasing existing database...")
        os.remove(GLOBALS["db_file"])
        
    # Initialize db if necessary, otherwise extract basic info
    if not os.path.exists(GLOBALS["db_file"]):
        db.init_db(os.path.join(GLOBALS["db_file"]))
        check_and_load_sequences(args, base_dir, npr_template)
    else:
        db.init_db(os.path.join(GLOBALS["db_file"]))
        log.warning("Skipping check and load sequences (using existent database)")
        if npr_template == "sptree":
            if args.spfile:
                if GLOBALS["target_species"] - db.get_all_species():
                    raise DataError("Not all species are present in the database")
            else:
                GLOBALS["target_species"] = db.get_all_species()
            log.log(28, "Working on %d species", len(GLOBALS["target_species"]))
        elif npr_template == "genetree":
            if "aa" in GLOBALS["seqtypes"]:
                GLOBALS["target_sequences"] = db.get_all_seqids("aa")
            else:
                GLOBALS["target_sequences"] = db.get_all_seqids("nt")
            
    # how task will be executed
    if args.execute:
        if args.nodetach:
            execution = ("insitu", False)
        else:
            execution =("insitu", True)
    elif args.sge_execute:
        execution = ("sge", False)
    else:
        execution = (None, False)

    # Scheduling starts here
    GLOBALS["config"] = config
    schedule(pipeline, args.schedule_time,
             execution, args.retry, args.debug)

    if args.compress:
        log.log(20, "Compressing files from intermediate steps...")
        cmd = "cd %s && find -t d -maxdepth 1 -exec tar -jcf {}.tar.bz2 {} \;" %\
              GLOBALS["basedir"]
        print cmd


def check_and_load_sequences(args, base_dir, npr_template):
    # Load ortho pairs
    all_species = set()
    target_seqs = set()
    seq2username = {}
    if npr_template == "sptree" and args.ortho_pairs:
        log.log(28, "Loading ortho pairs into database...")
        for line in open(args.ortho_pairs):
            try:
                name1, name2, pair_score = map(strip, line.split("\t"))
            except ValueError:
                name1, name2 = map(strip, line.split("\t"))
                pair_score = 1.0
            tax1, seq1 = map(strip, name1.split(args.spname_delimiter))
            tax2, seq2 = map(strip, name2.split(args.spname_delimiter))

            if not GLOBALS["target_species"] or \
               set([tax1,tax2]).issubset(GLOBALS["target_species"]):
                # add ortho pair to DB
                (tax1, seq1), (tax2, seq2) = sorted([(tax1,seq1), (tax2,seq2)])
                db.add_ortho_pair(tax1, seq1, tax2, seq2, pair_score)
                # keep a list of target seqs and species
                all_species.update([tax1, tax2])
                for realname in name1, name2:
                    name = realname
                    if args.seq_rename:
                        codename = encode_seqname(realname)
                        seq2username[codename] = name
                        name = codename
                    target_seqs.add(name)
        db.commit()
        log.log(28, "%d pairs uploaded." %len(target_seqs))
        if GLOBALS["target_species"]: 
            if GLOBALS["target_species"] - all_species:
                log.warning("No orthologous pairs were found for the following species: %s", 
                            GLOBALS["target_species"] - all_species)
            elif all_species - GLOBALS["target_species"]:
                ValueError("Inconsistency found when loading species. Please report.")
        else: 
            GLOBALS["target_species"] = all_species
            
    visited_seqs = {"aa":[], "nt":[]}
    seq2length = {"aa":{}, "nt":{}}
    seq2unknown = {"aa":{}, "nt":{}}
    seq2seq = {"aa":{}, "nt":{}}
    skipped_seqs = 0
    for seqtype in ["aa", "nt"]:
        seqfile = getattr(args, "%s_seed_file" %seqtype)
        if not seqfile:
            continue
        GLOBALS["seqtypes"].add(seqtype)
        SEQS = SeqGroup(seqfile, fix_duplicates=False)
        for seqid, seq in SEQS.id2seq.iteritems():
            seqname = SEQS.id2name[seqid]
            if args.seq_rename:
                # This is a bit tricky. I want to generate 10 char
                # unique names that are the always the same for
                # identical original names. Hash functions cannot
                # generate less than 16 bits strings, so I will
                # trust in the first 10 chars. It is very unlikely
                # to get a collision even in large sets (million
                # of names.), but in such a case, I will raise an
                # error.
                username = seqname
                seqname = encode_seqname(seqname)
            elif len(n)>10:
                seqname = seqname[:10]
                log.warning("sequence name %s was truncated to 10"
                            " chars: [%s]" %(n, n[:10]))
                
            if target_seqs and seqname not in target_seqs:
                skipped_seqs += 1
                continue
            visited_seqs[seqtype].append(seqname)
            
            if args.seq_rename:
                seq2username[seqname] = username
            
            # Clear symbols
            seq = seq.replace(".", "-")
            seq = seq.replace("*", "X")
            if args.unalign:
                seq = seq.replace("-", "").replace(".", "")
            seq2seq[seqtype][seqname] = seq
            seq2length[seqtype][seqname] = len(seq)
            # Load unknown symbol inconsistencies 
            if seqtype == "nt" and set(seq) - NT:
                seq2unknown[seqtype][seqname] = set(seq) - NT
            elif seqtype == "aa" and set(seq) - AA:
                seq2unknown[seqtype][seqname] = set(seq) - AA

        # Initialize target sets using aa as source
        if not target_seqs and seqtype == "aa":
            target_seqs = set(visited_seqs["aa"])
                
    if skipped_seqs:
        log.warning("%d sequences will not be used since they do not" 
                    " match any orthologous pair or are not present"
                    " in the aa seed file." %skipped_seqs)
        
    error = ""
    trans_name = lambda _n: seq2username.get(_n, _n)
    source_seqtype = "aa" if "aa" in GLOBALS["seqtypes"] else "nt"
    
    # Check for duplicate ids
    seq_number = len(set(visited_seqs[source_seqtype]))
    if len(visited_seqs[source_seqtype]) != seq_number:
        plainname_number = len(set(seq2username.values()))
        if args.seq_rename and plainname_number != seq_number:
            error += ("Although collisions in the hash function"
                      " used to generate unique sequence names" 
                      " are very unlikely, you have found an " 
                      " exception!. Please, change seq names " 
                      " manually.")
        else:
            counter = defaultdict(int)
            for seqname in visited_seqs["aa"]:
                counter[seqname] += 1
            duplicates = ["%s\thas %d copies" %(trans_name(key), value) for key, value in counter.iteritems() if value > 1]
            error += "\nDuplicate sequence names.\n"
            error += '\n'.join(duplicates)

    # in sptree workflow, check that the seq of all orthologs is available
    for seqtype in GLOBALS["seqtypes"]:
        missing_orth = target_seqs - set(seq2seq[seqtype].keys())
        if missing_orth:
            error += "\nThe following %s sequences are missing:\n" %seqtype
            error += '\n'.join(map(trans_name, missing_orth))
        
    # check for unknown characters         
    for seqtype in GLOBALS["seqtypes"]:
        if seq2unknown[seqtype]:
            error += "\nThe following %s sequences contain unknown symbols:\n" %seqtype
            error += '\n'.join(["%s\tcontains:\t%s" %(trans_name(k),' '.join(v)) for k,v in seq2unknown[seqtype].iteritems()] )
            
    # check for aa/cds consistency
    if GLOBALS["seqtypes"] == set(["aa", "nt"]):
        missing_nt = set(visited_seqs["aa"]) - set(visited_seqs["nt"])
        if missing_nt: 
            error += "\nThe following nucleotide sequences could not be found:\n"
            error += '\n'.join(map(trans_name, missing_nt))
           
        inconsistent_cds = set()
        for seqname, ntlen in seq2length["nt"].iteritems():
            if seqname in seq2length["aa"]:
                aa_len = seq2length["aa"][seqname]
                if ntlen / 3.0 != aa_len:
                    inconsistent_cds.add("%s\tExpected:%d\tFound:%d" %\
                                         (trans_name(seqname),
                                         aa_len*3,
                                         ntlen))
        if inconsistent_cds:
            error += "\nUnexpected coding sequence length for the following ids:\n"
            error += '\n'.join(inconsistent_cds)

    # Show some stats
    all_len = seq2length[source_seqtype].values()
    max_len = numpy.max(all_len)
    min_len = numpy.min(all_len)
    mean_len = numpy.mean(all_len)
    std_len = numpy.std(all_len)
    outliers = []
    for v in all_len:
        if abs(mean_len - v) >  (3 * std_len):
            outliers.append(v)
    log.log(28, "Max sequence length:  %d" %max_len)
    log.log(28, "Min sequence length:  %d" %min_len)
    log.log(28, "Mean sequence length: %d +- %0.1f " %(mean_len, std_len))
    if outliers:
        log.warning("%d sequence lengths look like outliers" %len(outliers))
        
    # Load data
    if error:
        OUT = open("npr_error.log", "w")
        OUT.write(' '.join(sys.argv) + "\n\n")
        OUT.write(error)
        OUT.close()
        raise DataError("Errors were found while loading data. Please"
                        " check error file in the current director for details"
                        " [npr_error.log]")
    else:
        log.log(28, "Loading %d sequence name translations..." %len(seq2username))
        for k, v in seq2username.iteritems():
            db.add_seq_name(k, v)            
        for seqtype in GLOBALS["seqtypes"]:
            log.log(28, "Loading %d %s sequences..." %(len(seq2seq[seqtype]), seqtype))
            for k, seq in seq2seq[seqtype].iteritems():
                db.add_seq(k, seq, seqtype)
        db.commit()
    if npr_template == "genetree":
        GLOBALS["target_sequences"] = set(visited_seqs[source_seqtype])


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description=__DESCRIPTION__,
                                     formatter_class=argparse.RawDescriptionHelpFormatter)
    # name or flags - Either a name or a list of option strings, e.g. foo or -f, --foo.
    # action - The basic type of action to be taken when this argument is encountered at the command line. (store, store_const, store_true, store_false, append, append_const, version)
    # nargs - The number of command-line arguments that should be consumed. (N, ? (one or default), * (all 1 or more), + (more than 1) )
    # const - A constant value required by some action and nargs selections.
    # default - The value produced if the argument is absent from the command line.
    # type - The type to which the command-line argument should be converted.
    # choices - A container of the allowable values for the argument.
    # required - Whether or not the command-line option may be omitted (optionals only).
    # help - A brief description of what the argument does.
    # metavar - A name for the argument in usage messages.
    # dest - The name of the attribute to be added to the object returned by parse_args().


    # Input data related flags
    input_group = parser.add_argument_group('INPUT OPTIONS\n==============')

    input_group.add_argument("-w", dest="workflow",
                        choices=["sptree", "genetree"],
                        required=True,
                        help="Phylogenetic workflow to follow.")

    input_group.add_argument("-c", "--config", dest="configfile",
                        type=existing_file, required=True,
                        help="NPR configuration file.")
    
    input_group.add_argument("-a", dest="aa_seed_file",
                        type=existing_file,
                        help="Initial multi sequence file with"
                        " protein sequences.")

    input_group.add_argument("-n", dest="nt_seed_file",
                        type=existing_file,
                        help="Initial multi sequence file with"
                        " nucleotide sequences")

    input_group.add_argument("--dealign", dest="unalign",
                        action="store_true",
                        help="If used, gaps in the orginal fasta file will"
                        " be removed (allows to use alignment files as input.")
    
    input_group.add_argument("--no-seq-rename", dest="seq_rename",
                        action="store_false",
                        help="If used, sequence names will NOT be"
                             " internally translated to 10-character-"
                             "identifiers.")
    
    # Sptree workflow
    
    input_group.add_argument("--ortho-pairs", dest="ortho_pairs",
                        type=existing_file,
                        help="File containing all one2one orthologous"
                             " relationships among the species of interest.")

    input_group.add_argument("--spname-delimiter", dest="spname_delimiter",
                        type=str, default="_",
                        help="In sptree mode, spname_delimiter is used to split"
                             " the name of sequences into species code and"
                             " sequence identifier (i.e. HUMAN_p53 = HUMAN, p53)."
                             " Note that species name must always precede seq.identifier.")

    input_group.add_argument("--spfile", dest="spfile",
                        type=existing_file,
                        help="If specified, only the sequences and orthologous"
                             " pairs matching the group of species in this file"
                             " (one species code per line) will be used. ")
   
    
    # Output data related flags
    output_group = parser.add_argument_group('OUTPUT OPTIONS\n==============')
    output_group.add_argument("-o", "--outdir", dest="outdir",
                        type=str, required=True,
                        help="""Output directory for results.""")

    output_group.add_argument("--compress", action="store_true",
                        help="Compress intermediate files when"
                              " tasks are finished.")
    
    output_group.add_argument("--logfile", action="store_true",
                          help="Log messages are saved into a file"
                      )
    
    # Task execution related flags
    exec_group = parser.add_argument_group('INPUTEXECUTION OPTIONS\n======================')

    exec_group.add_argument("-r", "--retry_error_jobs", dest="retry",
                        action="store_true",
                        help="Try to relaunch, once more, jobs marked"
                        " as error.")

    exec_group.add_argument("-m", "--maxcores", dest="maxcores", type=int,
                        default=1, help="Maximum number of CPU cores"
                        " available in the execution hosts. If higher"
                        " than 1, tasks with multi-threading"
                        " capabilities will be set appropriately.")

    exec_group.add_argument("-t", "--schedule_time", dest="schedule_time",
                        type=float, default=30.0,
                        help="""Time between task checks.""")

    exec_type_group = exec_group.add_mutually_exclusive_group()
    exec_type_group.add_argument("-x", "--execute_insitu", dest="execute",
                        action="store_true",
                        help="Jobs are launched in the same machine"
                        " as in the program is called.")

    exec_type_group.add_argument("--sge", dest="sge_execute",
                        action="store_true", help="Jobs will be"
                        " launched using the Sun Grid Engine"
                        " queue system.")

    exec_group.add_argument("--nodetach", dest="nodetach",
                        action="store_true", help="Jobs will NOT be"
                        " detached from the main NPR process. This means that"
                        " when npr execution is interrupted, all currently"
                        " running jobs will die with it. Use this option if you"
                        " want to execute the npr program as single shot job"
                        " (i.e. send npr jobs to a cluster). Note that"
                        " by default, npr is executed in monitor mode, so"
                        " launched jobs keeps running even when npr"
                        " process is killed." )

    exec_group.add_argument("--override", dest="override",
                            action="store_true",
                            help="Overrides output files from past program executions." )

    exec_group.add_argument("--cleardb", dest="cleardb",
                            action="store_true",
                            help="Overrides execution database in case it exists in"
                            " the output directory. This implies reloading all"
                            " sequence information. Files from finished tasks are"
                            " are kept, thus allowing resume previous executions)")
    
    exec_group.add_argument("--arch", dest="arch",
                            choices=["auto", "32", "64"],
                            default="auto", help="Set the architecture of"
                            " execution hosts (needed only when using"
                            " built-in applications.)")

    # Interface related flags
    ui_group = parser.add_argument_group("INTERFACE OPTIONS\n=================")
    ui_group.add_argument("-u", dest="enable_ui",
                        action="store_true", help="When used, a color"
                        " based interface is launched to monitor NPR"
                        " processes. This feature is EXPERIMENTAL and"
                        " requires NCURSES libraries installed in your"
                        " system.")

    ui_group.add_argument("-v", dest="verbosity",
                        default = 0, type=int, choices=[0,1,2,3,4],
                        help="Verbosity level: 0=very quiet, 4=very "
                          " verbose.")

    ui_group.add_argument("--nochecks", dest="nochecks",
                          action="store_true",
                          help="Skip application checks during the start-up")

    ui_group.add_argument("--debug", nargs="?",
                          const="all",
                          help="Starts a debugging"
                          " session. A taskid can be provided, so"
                          " debugging will start from such task on."
                          )

    args = parser.parse_args()

    if not args.aa_seed_file and not args.nt_seed_file:
        parser.error('At least one input file argument (-a, -n) is required')

    GLOBALS["basedir"] = os.path.abspath(args.outdir)
    GLOBALS["runid"] = generate_runid()
    GLOBALS["nodeinfo"] = defaultdict(dict)
    GLOBALS["threadinfo"] = defaultdict(dict)
    GLOBALS["seqtypes"] = set()
    GLOBALS["target_species"] = set()
    GLOBALS["target_sequences"] = set()
    GLOBALS["spname_delimiter"] = args.spname_delimiter
    GLOBALS["color_shell"] = True
    
    if not os.path.exists(GLOBALS["basedir"]):
        os.makedirs(GLOBALS["basedir"])

    # Start the application
    app_wrapper(main, args)
