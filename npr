#!/usr/bin/env python
# -*- coding: utf-8 -*-

import errno
import __builtin__
def wrap(method, retries):
    def fn(*args, **kwargs):
        for i in xrange(retries):
            try:
                return method(*args, **kwargs)
            except IOError, e:            
                if e.errno == errno.EINTR:
                    print >>sys.stderr, "A system call interruption was captured"
                    print >>sys.stderr, "Retrying", i, "of", retries, "until exception is raised"
                    continue
                else:
                    raise
    fn.retries = retries
    return fn

class safefile(file):
    __retries = 2
    def read(self, *args, **kargs):
        print "CAPTURADO"
        for i in xrange(self.__retries):
            try:
                return file.read(self, *args, **kargs)
            except IOError, e:            
                if e.errno == errno.EINTR:
                    print >>sys.stderr, "A system call interruption was captured"
                    print >>sys.stderr, "Retrying", i, "of", self.__retries, "until exception is raised"
                    continue
                else:
                    raise
    def write(self, *args, **kargs):
        for i in xrange(self.__retries):
            try:
                return file.read(self, *args, **kargs)
            except IOError, e:            
                if e.errno == errno.EINTR:
                    print >>sys.stderr, "A system call interruption was captured"
                    print >>sys.stderr, "Retrying", i, "of", self.__retries, "until exception is raised"
                    continue
                else:
                    raise
                
__builtin__.file = safefile
__builtin__.raw_input = wrap(raw_input, 100)
 
import sys
import os
import shutil
from StringIO import StringIO
from collections import defaultdict
import filecmp
import logging
from tempfile import NamedTemporaryFile        
log = None
import numpy
from time import ctime, time
# This avoids installing nprlib module. npr script will find it in the
# same directory in which it is
NPRPATH = os.path.split(os.path.realpath(__file__))[0]
sys.path.insert(0, NPRPATH)

from nprlib import argparse
from nprlib.configobj import ConfigObj, flatten_errors
from nprlib import validate
from nprlib.utils import (strip, SeqGroup, generate_runid,  AA, NT,
                          GLOBALS, encode_seqname, pjoin, pexist,
                          hascontent, checksum, ETE_CITE)
from nprlib.errors import ConfigError, DataError
from nprlib.master_task import Task
from nprlib.interface import app_wrapper
from nprlib.scheduler import schedule
from nprlib import db
from nprlib import apps
from nprlib.template.common import CONFIG_SPECS
from nprlib.logger import logindent
from nprlib.citation import Citator

APPSPATH =  pjoin(NPRPATH, "ext_apps/")

try:
    module_path = os.path.split(__file__)[0]
    __VERSION__ = open(os.path.join(module_path, "VERSION")).read().strip()
except:
    __VERSION__ = "unknown"

try:
    module_path = os.path.split(__file__)[0]
    __DATE__ = open(os.path.join(module_path, "DATE")).read().strip()
except:
    __DATE__ = "unknown"

__DESCRIPTION__ = (
"""--------------------------------------------------------------------------------
            Nested Phylogenetic Reconstruction (NPR) program.
                          NPR-0.1%s(beta), %s.
    
      NPR allows to reconstruct large phylogenies by means of an
      iterative strategy that provides scalable resolution along the
      different levels of a tree. At each iteration, internal nodes
      are re-evaluated to optimize and fine tune the parameters of the
      phylogenetic inference.

      If you use this program in a published work, please cite:

        Jaime Huerta-Cepas and Toni Gabaldon. Nested Phylogenetic
        Reconstruction. XXX-XX. 

      (Note that a list of the external programs used to complete the
      necessary computations will be shown shown together with your
      results.)

      Contact: jhuerta [at] crg.es & tgabaldon [at] crg.es
--------------------------------------------------------------------------------

    """ %(__VERSION__, __DATE__))

def existing_file(value):
    if os.path.isfile(value):
        return value
    else:
        raise ConfigError("Not valid file")

def existing_dir(value):
    if os.path.isdir(value):
        return value
    else:
        raise ConfigError("Not valid file")
        
def check_number(value, cast, minv=0, maxv=None):
    try:
        typed_value = cast(value)
    except ValueError:
        raise ConfigError("Expected [%s] number. Found [%s]" %(cast, value))
    else:
        if (minv is not None and typed_value < cast(minv)) or \
           (maxv is not None and typed_value > cast(maxv)):
            _minv = minv if minv is not None else "any"
            _maxv = maxv if maxv is not None else "any"
            raise ConfigError("[%s] not in the range (%s,%s)" %
                              (value, _minv, _maxv))
    return typed_value

def is_float(value, minv=None, maxv=None):
    return check_number(value, float, minv, maxv)

def is_integer(value, minv=None, maxv=None):
    return check_number(value, int, minv, maxv)

def is_list(value):
    if not isinstance(value, list):
        raise ConfigError("Expected a list of values. Found [%s]" %value)
    return value

def is_boolean_list(value):
    is_list(value)
    typed_value = []
    for v in value:
        typed_value.append(check_number(v, bool, 0, 1))
    return typed_value

def is_float_list(value, minv=0, maxv=None):
    is_list(value)
    typed_value = []
    for v in value:
        typed_value.append(check_number(v, float, minv, maxv))
    return typed_value

def is_integer_list(value, minv=0, maxv=None):
    is_list(value)
    typed_value = []
    for v in value:
        typed_value.append(check_number(v, int, minv, maxv))
    return typed_value

def is_correlative_integer_list(value, minv=1, maxv=None):
    is_list(value)
    typed_value = []
    last_value = 0
    for v in value:
        typed_value.append(check_number(v, int, minv, maxv))
        if int(v) <= last_value:
            raise ConfigError("Numeric values are not correlative: %s" %value)
        last_value = int(v)
    return typed_value

def main(args):
    global log
    base_dir = GLOBALS["basedir"]
    log = logging.getLogger("main")
    print __DESCRIPTION__
    
    npr_template = args.workflow
    if npr_template == "sptree":
        GLOBALS["template"] = "sptree"
        from nprlib.template.sptree import pipeline
    elif npr_template == "genetree":
        GLOBALS["template"] = "genetree"
        from nprlib.template.genetree import pipeline
    else:
        raise ConfigError("Bad config file. A template name or file should be specified.")
    
    if args.arch == "auto":
        arch = "64 " if sys.maxsize > 2**32 else "32"
    else:
        arch = args.arch
        
    # UnCompress packed execution data
    if pexist(os.path.join(base_dir,"nprdata.tar.bz2")):
        log.log(28, "compressed data found. Extracting content to start execution...")
        cmd = "cd %s && tar -jxf nprdata.tar.bz2 && rm nprdata.tar.bz2" % base_dir
        os.system(cmd)

    # Check and load all configuration files
    run2config = {}
    for fname in args.configfile:
        clearname = os.path.basename(fname)
        local_conf_file = pjoin(base_dir, clearname, "workflow.cfg")
        if pexist(base_dir):
            if hascontent(local_conf_file):
                if not filecmp.cmp(fname, local_conf_file):
                    if not args.override:
                        raise ConfigError("Output directory seem to contain"
                                      " a NPR execution using a different"
                                      " config file [workflow.cfg]. Use"
                                      " --override option or change the"
                                      " output path.")

        try:
            config = ConfigObj(fname, list_values=True,
                               configspec=StringIO(CONFIG_SPECS))
        except Exception, e:
            raise ConfigError(str(e))

        custom_types = {"boolean_list": is_boolean_list,
                        "integer_list": is_integer_list,
                        "correlative_integer_list": is_correlative_integer_list,
                        "float_list": is_float_list,
                        "float": is_float,
                        "integer": is_integer,
                        "list": is_list,
                        }
        val = validate.Validator(custom_types)
        check = config.validate(val)
        if not check:
            print flatten_errors(config, check)
            raise ConfigError("Errors found when parsing config file")

        # Check that fields are filled for all max_seq values
        for flow_name in ["sptree", "genetree"]:
            npr_config = [len(v) for k, v in config[flow_name].iteritems()
                          if type(v) == list]
            if len(set(npr_config)) != 1:
                print '\n'.join([str((len(v),v)) for k, v in config[flow_name].iteritems()
                          if type(v) == list])
                raise ConfigError("Unequal length of main NPR config table. Missing comma?")

        # setup portable apps
        apps_to_test = {}
        for k,v in config["app"].iteritems():
            if k in config["threading"]:
                core_limit = int(config["threading"][k])
                config["threading"][k] = min(core_limit, args.maxcores)
               
            if v == "built-in":
                cores = int(config["threading"].get(k, args.maxcores))
                #cores = min(args.maxcores, cores)
                config["threading"][k] = cores
                if args.sge_execute:
                    _apps_path = os.path.realpath(config["sge"]["remote_npr_path"])
                    cmd = apps.get_call(k, _apps_path, base_dir, cores)
                    config["app"][k] = cmd
                    apps_to_test[k] = cmd
                else:
                    cmd = apps.get_call(k, APPSPATH, base_dir, str(cores))
                    config["app"][k] = cmd
                    apps_to_test[k] = cmd

        # Copy config file
        config["_outpath"] = pjoin(base_dir, clearname)
        try:
            os.makedirs(config["_outpath"])
        except OSError:
            pass
        shutil.copy(fname, local_conf_file)

        # save checksum of current workflow
        config["_config_checksum"] = checksum(local_conf_file)
        run2config[clearname] = config
            
    # Create main dir structure
    gallery_dir = os.path.join(base_dir, "gallery")
    sge_dir = os.path.join(base_dir, "sge_jobs")
    tmp_dir = os.path.join(base_dir, "tmp")
    tasks_dir = os.path.join(base_dir, "tasks")
    input_dir = os.path.join(base_dir, "input")
    GLOBALS["sge_dir"] = sge_dir
    GLOBALS["tmp"] = tmp_dir
    GLOBALS["gallery_dir"] = gallery_dir
    GLOBALS["tasks_dir"] = tasks_dir
    GLOBALS["input_dir"] = input_dir
    for dirname in [tmp_dir, tasks_dir, input_dir]:
        try:
            os.makedirs(dirname)
        except OSError:
            log.warning("Using existing dir: %s", dirname)


    if not args.nochecks:
        print "Testing x86-%s portable applications..." % arch
        apps.test_apps(apps_to_test)
    
    # Set number of CPUs available
    GLOBALS["_max_cores"] = args.maxcores
    log.debug("Enabling %d CPU cores" %args.maxcores)
            
    
    # if npr_template == "sptree" and not args.ortho_pairs:
    #     raise ConfigError("Species tree workflow requires a list"
    #                       " of orthologous gene/protein pairs to be"
    #                       " supplied through the --ortho-pair"
    #                       " argument.")
    
    if npr_template == "sptree" and not args.cogs_file:
        raise ConfigError("Species tree workflow requires a list of COGS"
                          " to be supplied through the --cogs"
                          " argument.")

    GLOBALS["seqtypes"] = set()
    if args.nt_seed_file:
        GLOBALS["seqtypes"].add("nt")
        GLOBALS["inputname"] = os.path.split(args.nt_seed_file)[-1]
        
    if args.aa_seed_file:
        GLOBALS["seqtypes"].add("aa")
        GLOBALS["inputname"] = os.path.split(args.aa_seed_file)[-1]

    # Set up custom databases if necessary
    GLOBALS["nprdb_file"]  = pjoin(base_dir, "npr.%s.db" %time())
   
    GLOBALS["datadb_file"]  = pjoin(base_dir, "data.db")
    GLOBALS["orthodb_file"]  = pjoin(base_dir, "ortho.db") \
        if not args.orthodb else args.orthodb
    GLOBALS["seqdb_file"]  = pjoin(base_dir, "seq.db") \
        if not args.seqdb else args.seqdb

    # Clear databases if necessary
    if args.cleardb and pexist(GLOBALS["nprdb_file"]):
        log.log(28, "Erasing existing npr database...")
        os.remove(GLOBALS["nprdb_file"])
    if args.clearseqs and pexist(GLOBALS["seqdb_file"]):
        log.log(28, "Erasing existing seq database...")
        os.remove(GLOBALS["seqdb_file"])
    if args.clearorthology and pexist(GLOBALS["orthodb_file"]):
        log.log(28, "Erasing existing ortho database...")
        os.remove(GLOBALS["orthodb_file"])
        
    # Initialize db if necessary, otherwise extract basic info
    db.init_nprdb(GLOBALS["nprdb_file"])
    db.init_datadb(GLOBALS["datadb_file"])
    
    # Check and load data
    ERROR = ""
    if not pexist(GLOBALS["seqdb_file"]):
        db.init_seqdb(GLOBALS["seqdb_file"])
        target_seqs = None
        target_seqs, seqnames, name2len, name2unk, name2seq = scan_sequences(args, target_seqs)
        ERROR = check_seq_integrity(target_seqs, seqnames, name2len, name2unk, name2seq)
        sp2counter = defaultdict(int)
        if npr_template == "sptree":
            for name in target_seqs:
                spname = name.split(GLOBALS["spname_delimiter"])[0]
                sp2counter[spname] += 1
                #seq_species = set([name.split(GLOBALS["spname_delimiter"])[0] 
                #               for name in target_seqs])
            db.add_seq_species(sp2counter)
            seq_species = set(sp2counter.keys())
        if not ERROR: 
            load_sequences(target_seqs, name2seq, npr_template)
        seqnames, name2len, name2unk, name2seq = [None] * 4 # Release mem
    else:
        db.init_seqdb(GLOBALS["seqdb_file"])
        log.warning("Skipping check and load sequences (loading from database...)")
        target_seqs = db.get_all_seq_names()
        
    log.warning("%d sequences in database" %len(target_seqs))

    if npr_template == "sptree":
        if not pexist(GLOBALS["orthodb_file"]):
             db.init_orthodb(GLOBALS["orthodb_file"])            
        #     check_and_load_orthologs(args.ortho_pairs)
        #     log.log(28, "Updating species info in ortholog-pairs")
        #     db.update_species_in_ortho_pairs()
        #     db.orthoconn.commit()
             all_species = set()
             for line in open(args.cogs_file):
                 all_species.update(map(lambda n: n.split("_")[0].strip(), line.split("\t")))
             db.update_cog_species(all_species)
             print len(all_species)
             db.orthoconn.commit()
        else:
            db.init_orthodb(GLOBALS["orthodb_file"])
            log.warning("Skipping check and load ortholog pairs (loading from database...)")
        
        # species in ortho pairs
        ortho_species = db.get_ortho_species()
        log.log(28, "Found %d unqiue species codes in ortholog-pairs" %(len(ortho_species)))
        # species in fasta file
        seq_species = db.get_seq_species()
        log.log(28, "Found %d unqiue species codes in sequence names" %(len(seq_species))) 
        
        # Species filter
        if args.spfile:
            target_species = set([line.strip() for line in open(args.spfile)])
            target_species.discard("")
            log.log(28, "Enabling NPR for %d species", len(target_species))
        else: 
            target_species = seq_species

        # Check that orthologs are available for all species
        if target_species - seq_species: 
            ERROR += "The following species have no sequence information: %s" %(target_species - seq_species)
        if target_species - ortho_species: 
            ERROR += "The following species have no orthology information: %s" %(target_species - ortho_species)
            
        GLOBALS["target_species"] = target_species
        
    if ERROR:
        open("npr_error.log", "w").write(' '.join(sys.argv) + "\n\n" + ERROR)
        raise DataError("Errors were found while loading data. Please"
                        " check error file in the current director for details"
                        " [npr_error.log]")

    target_seqs = None # release mem

    if npr_template == "genetree":
        if "aa" in GLOBALS["seqtypes"]:
            GLOBALS["target_sequences"] = db.get_all_seqids("aa")
        else:
            GLOBALS["target_sequences"] = db.get_all_seqids("nt")
        log.log(28, "Working on %d ids whose sequences are available", len(GLOBALS["target_sequences"]))
            
    # how task will be executed
    if args.execute:
        if args.monitor:
            execution =("insitu", True) # True is for run-detached flag
        else:
            execution = ("insitu", False)

    elif args.sge_execute:
        execution = ("sge", False)
    else:
        execution = (None, False)
       
    # Scheduling starts here
    log.log(28, "NPR starts now!")
    
    # This initialises all pipelines
    pending_tasks = []
    start_time = ctime()
    for name, config in run2config.iteritems():
        # Feeds pending task with the first task of the workflow
        config["_name"] = name
        new_tasks = pipeline(None, config)
        thread_id = new_tasks[0].threadid
        config["_configid"] = thread_id
        GLOBALS[thread_id] = config
        pending_tasks.extend(new_tasks)

        # Clear info from previous runs
        open(os.path.join(config["_outpath"], "runid"), "a").write('\t'.join([thread_id, GLOBALS["nprdb_file"]+"\n"]))
        # Write command line info
        cmd_info = '\t'.join([start_time, thread_id, GLOBALS["cmdline"]])
        open(pjoin(config["_outpath"], "command_lines"), "a").write(cmd_info+"\n")

    schedule(pipeline, pending_tasks, args.schedule_time,
             execution, args.retry, args.debug)
    db.close()
    if args.compress:
        log.log(20, "Compressing intermediate data...")
        cmd = "cd %s && tar --remove-files -jcf nprdata.tar.bz2 ortho.db seq.db tasks/" %\
              GLOBALS["basedir"]
        os.system(cmd)

def check_and_load_orthologs(fname):
    log.log(28, "importing orthologous pairs into database (this may take a while)...")
    template_import = NamedTemporaryFile()
    template_import.write('''
PRAGMA cache_size = 1000000;
PRAGMA synchronous = OFF;
PRAGMA journal_mode = OFF;
PRAGMA locking_mode = EXCLUSIVE;
PRAGMA temp_store = MEMORY;
PRAGMA auto_vacuum = NONE;
.separator "\\t"
.import %s ortho_pair
PRAGMA locking_mode = EXCLUSIVE;

CREATE INDEX IF NOT EXISTS i7 ON ortho_pair (taxid2, seqid2, taxid1);
CREATE INDEX IF NOT EXISTS i10 ON ortho_pair (taxid1, taxid2);

PRAGMA synchronous = NORMAL;
PRAGMA journal_mode = DELETE;
PRAGMA locking_mode = NORMAL;

''' %  fname)

    template_import.flush()
    cmd = "sqlite3 %s < %s" %(GLOBALS["orthodb_file"], template_import.name)
    print cmd
    os.system(cmd)
    template_import.close()
    

def load_sequences(target_seqs, name2seq, npr_template):
    if args.seq_rename:
        name2hash, hash2name = hash_names(target_seqs)
        log.log(28, "Loading %d sequence name translations..." %len(hash2name))
        db.add_seq_name_table(hash2name.items())
        if npr_template == "genetree":
            GLOBALS["target_sequences"] = hash2name.keys()
    else:
        name2hash, hash2name = {}, {}

    for seqtype in GLOBALS["seqtypes"]:
        log.log(28, "Loading %d %s sequences..." %(len(name2seq[seqtype]), seqtype))
        # Load sequences and replace U to avoid problems
        db.add_seq_table([(name2hash.get(k, k), seq.replace("U","B")) for k,seq in
                          name2seq[seqtype].iteritems()], seqtype)
    db.seqconn.commit()

def scan_sequences(args, target_seqs):
    visited_seqs = {"aa":[], "nt":[]}
    seq2length = {"aa":{}, "nt":{}}
    seq2unknown = {"aa":{}, "nt":{}}
    seq2seq = {"aa":{}, "nt":{}}
    skipped_seqs = 0
    for seqtype in ["aa", "nt"]:
        seqfile = getattr(args, "%s_seed_file" %seqtype)
        if not seqfile:
            continue
        GLOBALS["seqtypes"].add(seqtype)
        log.log(28, "Scanning %s sequence file...", seqtype)
        SEQS = SeqGroup(seqfile, fix_duplicates=False)
        for c1, (seqid, seq) in enumerate(SEQS.id2seq.iteritems()):
            if c1%10000 == 0:
                print >>sys.stderr, c1, "\r",

            seqname = SEQS.id2name[seqid]
            if target_seqs and seqname not in target_seqs:
                skipped_seqs += 1
                continue
            visited_seqs[seqtype].append(seqname)

            # Clear symbols
            seq = seq.replace(".", "-")
            seq = seq.replace("*", "X")
            if args.dealign:
                seq = seq.replace("-", "").replace(".", "")
            seq2seq[seqtype][seqname] = seq
            seq2length[seqtype][seqname] = len(seq)
            # Load unknown symbol inconsistencies
            if seqtype == "nt" and set(seq) - NT:
                seq2unknown[seqtype][seqname] = set(seq) - NT
            elif seqtype == "aa" and set(seq) - AA:
                seq2unknown[seqtype][seqname] = set(seq) - AA

        # Initialize target sets using aa as source
        if not target_seqs and seqtype == "aa":
            target_seqs = set(visited_seqs["aa"])

    if skipped_seqs:
        log.warning("%d sequences will not be used since they are"
                    "  not present in the aa seed file." %skipped_seqs)
                             
    return target_seqs, visited_seqs, seq2length, seq2unknown, seq2seq

def check_seq_integrity(target_seqs, visited_seqs, seq2length, seq2unknown, seq2seq):
    log.log(28, "Checking data consistency ...")
    source_seqtype = "aa" if "aa" in GLOBALS["seqtypes"] else "nt"
    error = ""

    # Check for duplicate ids
    seq_number = len(set(visited_seqs[source_seqtype]))
    if len(visited_seqs[source_seqtype]) != seq_number:
        counter = defaultdict(int)
        for seqname in visited_seqs[source_seqtype]:
            counter[seqname] += 1
        duplicates = ["%s\thas %d copies" %(key, value) for key, value in counter.iteritems() if value > 1]
        error += "\nDuplicate sequence names.\n"
        error += '\n'.join(duplicates)

    # check that the seq of all targets is available
    if target_seqs: 
        for seqtype in GLOBALS["seqtypes"]:
            missing_seq = target_seqs - set(seq2seq[seqtype].keys())
            if missing_seq:
                error += "\nThe following %s sequences are missing:\n" %seqtype
                error += '\n'.join(missing_seq)

    # check for unknown characters
    for seqtype in GLOBALS["seqtypes"]:
        if seq2unknown[seqtype]:
            error += "\nThe following %s sequences contain unknown symbols:\n" %seqtype
            error += '\n'.join(["%s\tcontains:\t%s" %(k,' '.join(v)) for k,v in seq2unknown[seqtype].iteritems()] )

    # check for aa/cds consistency
    if GLOBALS["seqtypes"] == set(["aa", "nt"]):
        inconsistent_cds = set()
        for seqname, ntlen in seq2length["nt"].iteritems():
            if seqname in seq2length["aa"]:
                aa_len = seq2length["aa"][seqname]
                if ntlen / 3.0 != aa_len:
                    inconsistent_cds.add("%s\tExpected:%d\tFound:%d" %\
                                         (seqname,
                                         aa_len*3,
                                         ntlen))
        if inconsistent_cds:
            error += "\nUnexpected coding sequence length for the following ids:\n"
            error += '\n'.join(inconsistent_cds)

    # Show some stats
    all_len = seq2length[source_seqtype].values()
    max_len = numpy.max(all_len)
    min_len = numpy.min(all_len)
    mean_len = numpy.mean(all_len)
    std_len = numpy.std(all_len)
    outliers = []
    for v in all_len:
        if abs(mean_len - v) >  (3 * std_len):
            outliers.append(v)
    log.log(28, "Max sequence length:  %d" %max_len)
    log.log(28, "Min sequence length:  %d" %min_len)
    log.log(28, "Mean sequence length: %d +- %0.1f " %(mean_len, std_len))
    if outliers:
        log.warning("%d sequence lengths look like outliers" %len(outliers))

    return error

def hash_names(target_names):
    """Given a set of strings of variable lengths, it returns their
    conversion to fixed and safe hash-strings.
    """
    # An example of hash name collision
    #test= ['4558_15418', '9600_21104', '7222_13002', '3847_37647', '412133_16266']
    #hash_names(test)

    log.log(28, "Generating safe sequence names...")
    hash2name = defaultdict(list)
    for c1, name in enumerate(target_names):
        print >>sys.stderr, c1, "\r",
        hash_name = encode_seqname(name)
        hash2name[hash_name].append(name)

    collisions = [(k,v) for k,v in hash2name.iteritems() if len(v)>1]
    #GLOBALS["name_collisions"] = {}
    if collisions:
        visited = set(hash2name.keys())
        for old_hash, coliding_names in collisions:
            logindent(2)
            log.log(20, "Collision found when hash-encoding the following gene names: %s", coliding_names)
            niter = 1
            valid = False
            while not valid or len(new_hashes) < len(coliding_names):
                niter += 1
                new_hashes = defaultdict(list)
                for name in coliding_names:
                    hash_name = encode_seqname(name*niter)
                    new_hashes[hash_name].append(name)
                valid = set(new_hashes.keys()).isdisjoint(visited)
                
            log.log(20, "Fixed with %d concatenations! %s", niter, ', '.join(['%s=%s' %(e[1][0], e[0]) for e in  new_hashes.iteritems()]))
            del hash2name[old_hash]
            hash2name.update(new_hashes)
            #GLOBALS["name_collisions"].update([(_name, _code) for _code, _name in new_hashes.iteritems()])
            logindent(-2)
    #collisions = [(k,v) for k,v in hash2name.iteritems() if len(v)>1]
    #log.log(28, "Final collisions %s", collisions )
    hash2name = dict([(k, v[0]) for  k,v in hash2name.iteritems()])
    name2hash = dict([(v, k) for  k,v in hash2name.iteritems()])
    return name2hash, hash2name


if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] == "check":
        APPSPATH =  os.path.join(NPRPATH, "ext_apps/")
        # setup portable apps
        config = {}
        for k in apps.builtin_apps:
            cmd = apps.get_call(k, APPSPATH, "/tmp", "1")
            print cmd
            config[k] = cmd
        apps.test_apps(config)
        sys.exit(0)
    
    parser = argparse.ArgumentParser(description=__DESCRIPTION__,
                                     formatter_class=argparse.RawDescriptionHelpFormatter)
    # name or flags - Either a name or a list of option strings, e.g. foo or -f, --foo.
    # action - The basic type of action to be taken when this argument is encountered at the command line. (store, store_const, store_true, store_false, append, append_const, version)
    # nargs - The number of command-line arguments that should be consumed. (N, ? (one or default), * (all 1 or more), + (more than 1) )
    # const - A constant value required by some action and nargs selections.
    # default - The value produced if the argument is absent from the command line.
    # type - The type to which the command-line argument should be converted.
    # choices - A container of the allowable values for the argument.
    # required - Whether or not the command-line option may be omitted (optionals only).
    # help - A brief description of what the argument does.
    # metavar - A name for the argument in usage messages.
    # dest - The name of the attribute to be added to the object returned by parse_args().


    # Input data related flags
    input_group = parser.add_argument_group('INPUT OPTIONS\n==============')

    input_group.add_argument("-w", dest="workflow",
                        choices=["sptree", "genetree"],
                        required=True,
                        help="Type of phylogenetic workflow.")

    input_group.add_argument("-c", "--config", dest="configfile",
                        type=str, required=True, nargs="+",
                        help="Configuration file.")

    input_group.add_argument("-a", dest="aa_seed_file",
                        type=existing_file,
                        help="Initial multi sequence file with"
                        " protein sequences.")

    input_group.add_argument("-n", dest="nt_seed_file",
                        type=existing_file,
                        help="Initial multi sequence file with"
                        " nucleotide sequences")

    input_group.add_argument("--dealign", dest="dealign",
                        action="store_true",
                        help="when used, gaps in the orginal fasta file will"
                        " be removed, thus allowing to use alignment files as input.")

    input_group.add_argument("--no-seq-rename", dest="seq_rename",
                        action="store_false",
                        help="If used, sequence names will NOT be"
                             " internally translated to 10-character-"
                             "identifiers.")

    input_group.add_argument("--first-split-outgroup", dest="first_split",
                             type=str,
                             help=("When used, it overrides first_split option"
                                  " in any tree merger config block in the"
                                   " config file."))

    input_group.add_argument("--orthodb", dest="orthodb",
                             type=str,
                             help="Uses a custom orthology-pair database file")

    input_group.add_argument("--seqdb", dest="seqdb",
                             type=str,
                             help="Uses a custom sequence database file")
    
    # Sptree workflow

    input_group.add_argument("--ortho-pairs", dest="ortho_pairs",
                        type=existing_file,
                        help="File containing all one2one orthologous"
                             " relationships among the species of interest.")

    input_group.add_argument("--cogs", dest="cogs_file",
                        type=existing_file,
                        help="")
    
    input_group.add_argument("--spname-delimiter", dest="spname_delimiter",
                        type=str, default="_",
                        help="In sptree mode, spname_delimiter is used to split"
                             " the name of sequences into species code and"
                             " sequence identifier (i.e. HUMAN_p53 = HUMAN, p53)."
                             " Note that species name must always precede seq.identifier.")

    input_group.add_argument("--spfile", dest="spfile",
                        type=existing_file,
                        help="If specified, only the sequences and ortholog"
                             " pairs matching the group of species in this file"
                             " (one species code per line) will be used. ")


    # Output data related flags
    output_group = parser.add_argument_group('OUTPUT OPTIONS\n==============')
    output_group.add_argument("-o", "--outdir", dest="outdir",
                        type=str, required=True,
                        help="""Output directory for results.""")

    output_group.add_argument("--taskdir", dest="taskdir",
                        type=existing_dir,
                        help="""Output directory for tasks related data""")
    
    output_group.add_argument("--compress", action="store_true",
                        help="Compress intermediate files when"
                              " a task is finished.")

    output_group.add_argument("--logfile", action="store_true",
                          help="Log messages are saved into a file"
                      )
    
    output_group.add_argument("--email", dest="email",
                              type=str, 
                              help="Send email info when errors or finished threads")
    
    output_group.add_argument("--email_report_time", dest="email_report_time",
                              type=int, default = 0, 
                              help="Send email reports while execution")


    # Task execution related flags
    exec_group = parser.add_argument_group('INPUT EXECUTION OPTIONS\n======================')

    exec_group.add_argument("-r", "--retry_error_jobs", dest="retry",
                        action="store_true",
                        help="Try to relaunch jobs marked"
                        " as error.")

    exec_group.add_argument("-m", "--maxcores", dest="maxcores", type=int,
                        default=1, help="Maximum number of CPU cores"
                        " available in the execution hosts. If higher"
                        " than 1, tasks with multi-threading"
                        " capabilities will be set appropriately.")

    exec_group.add_argument("-t", "--schedule_time", dest="schedule_time",
                        type=float, default=2,
                        help="""How often tasks should be checked""")
    
    exec_group.add_argument("--launch_time", dest="launch_time",
                        type=float, default=5,
                        help="""How often queued jobs should be launched""")

    exec_type_group = exec_group.add_mutually_exclusive_group()
    exec_type_group.add_argument("-x", "--insitu_execution", dest="execute",
                        action="store_true",
                        help="Pipeline jobs are launched in the same machine as the"
                        " as ETE-NPR is running.")

    exec_type_group.add_argument("--sge", dest="sge_execute",
                        action="store_true", help="EXPERIMENTAL!: Jobs will be"
                        " launched using the Sun Grid Engine"
                        " queue system.")

    exec_group.add_argument("--monitor", dest="monitor",
                        action="store_true", help="Pipeline jobs will be"
                            " detached from the main NPR process. This means that"
                            " when npr execution is interrupted, all currently"
                            " running jobs will keep running. Use this option if you"
                            " want to stop and recover an NPR execution thread or"
                            " if jobs are expected to be executed remotely."
                            )

    exec_group.add_argument("--override", dest="override",
                            action="store_true",
                            help="Overrides output files from past program executions." )

    exec_group.add_argument("--clear_nprdb", dest="cleardb",
                            action="store_true",
                            help="Overrides all tables in database")

    exec_group.add_argument("--clear_orthodb", dest="clearorthology",
                            action="store_true",
                            help="Overrides orthology pairs table in database")

    exec_group.add_argument("--clear_seqdb", dest="clearseqs",
                            action="store_true",
                            help="Overrides sequences table in database")

    exec_group.add_argument("--arch", dest="arch",
                            choices=["auto", "32", "64"],
                            default="auto", help="Set the architecture of"
                            " execution hosts (needed only when using"
                            " built-in applications.)")

    # Interface related flags
    ui_group = parser.add_argument_group("INTERFACE OPTIONS\n=================")
    ui_group.add_argument("-u", dest="enable_ui",
                        action="store_true", help="When used, a color"
                        " based interface is launched to monitor NPR"
                        " processes. This feature is EXPERIMENTAL and"
                        " requires NCURSES libraries installed in your"
                        " system.")

    ui_group.add_argument("-v", dest="verbosity",
                        default = 0, type=int, choices=[0,1,2,3,4],
                        help="Verbosity level: 0=very quiet, 4=very "
                          " verbose.")

    ui_group.add_argument("--nochecks", dest="nochecks",
                          action="store_true",
                          help="Skip application checks during the start-up")

    ui_group.add_argument("--debug", nargs="?",
                          const="all",
                          help="Starts a debugging"
                          " session. A taskid can be provided, so"
                          " debugging will start from such task on."
                          )

    args = parser.parse_args()

    if not args.aa_seed_file and not args.nt_seed_file:
        parser.error('At least one input file argument (-a, -n) is required')

    GLOBALS["basedir"] = os.path.abspath(args.outdir)
    if args.taskdir:
        GLOBALS["taskdir"] = os.path.realpath(args.taskdir)
    else:
        GLOBALS["taskdir"] = os.path.join(GLOBALS["basedir"], "tasks")
        
    if args.first_split:
        GLOBALS["first_split_outgroup"] = args.first_split

    GLOBALS["email"] = args.email
    GLOBALS["email_report_time"] = args.email_report_time
    GLOBALS["launch_time"] = args.launch_time
    GLOBALS["cmdline"] = ' '.join(sys.argv)
    GLOBALS["nodeinfo"] = defaultdict(dict)
    GLOBALS["threadinfo"] = defaultdict(dict)
    GLOBALS["seqtypes"] = set()
    GLOBALS["target_species"] = set()
    GLOBALS["target_sequences"] = set()
    GLOBALS["spname_delimiter"] = args.spname_delimiter
    GLOBALS["color_shell"] = True
    GLOBALS["citator"] = Citator()
    
    GLOBALS["cogs_file"] = args.cogs_file
    GLOBALS["citator"].add(ETE_CITE)
    if not pexist(GLOBALS["basedir"]):
        os.makedirs(GLOBALS["basedir"])

    # Start the application
    app_wrapper(main, args)
